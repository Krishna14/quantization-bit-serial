{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy of modelQuantize.py",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1nLeUxG0EFGzyezIRSb1wxGlIMyyfT_H-",
      "authorship_tag": "ABX9TyNHk5KXfj7QvNK7X+pIRhJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishna14/quantization-bit-serial/blob/main/modelQuantize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOCbDywfQ3rz"
      },
      "source": [
        "### This cell contains the code for the Quantizer. This quantizer uses asymmetric quantization from ```FP32``` to ```INTN``` bits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1mknVI90yiC"
      },
      "source": [
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class Quantizer(nn.Module):\n",
        "  \"\"\"\n",
        "    This class takes in any kind of input tensor.\n",
        "    Quantizes the tensor and returns it as the output for the next module to use\n",
        "  \"\"\"\n",
        "  def __init__(self, model_fp32, inp_tensor, numBits, debug_mode=False):\n",
        "    super(Quantizer, self).__init__()\n",
        "    self.input = inp_tensor\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.model_fp32 = model_fp32\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "    self.numBits = numBits\n",
        "    self._debug_mode = debug_mode\n",
        "\n",
        "  def quantizeTensor(self, input_tensor, numBits, isActivations=False, debug_mode=False):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        input_tensor - Input tensor\n",
        "        numBits - number of bits to quantize the value.\n",
        "        isActivations - Determines whether activation should be quantized or weights\n",
        "      Returns:\n",
        "        Finds the scale, zero_point and quantizes it between value based on the number of bits\n",
        "        Signed n bit representation.\n",
        "    \"\"\"\n",
        "    print(\"quantizeTensor: type(input_tensor) = {}\".format(type(input_tensor)))\n",
        "    minVal, maxVal = torch.min(input_tensor).item(), torch.max(input_tensor).item()\n",
        "    if isActivations:\n",
        "      assert len(input_tensor.shape) == 3, \"Shape of activations tensor isn't correct\"\n",
        "    else:\n",
        "      assert len(input_tensor.shape) == 4, \"Shape of weights tensor isn't correct\"\n",
        "    \n",
        "    print(\"Types are {} and {}\".format(type(minVal), type(maxVal)))\n",
        "    if debug_mode:\n",
        "      print(\"quantizeTensor: min and max values of the tensor are {} and {}\".format(minVal, maxVal))\n",
        "    divisor, shifter = ((2 ** numBits) - 1), 2**(numBits-1)\n",
        "    q_scale = float((maxVal - minVal)/divisor)\n",
        "    if input_tensor.requires_grad:\n",
        "      input_np = input_tensor.detach().numpy()\n",
        "    else:\n",
        "      input_np = input_tensor.numpy()\n",
        "    zero_point = round(minVal/q_scale)\n",
        "    for x in np.nditer(input_np,op_flags=['readwrite']):\n",
        "      x[...] = round(x/q_scale - zero_point) - shifter\n",
        "    output = torch.tensor(input_np)\n",
        "    assert output.shape == input_tensor.shape\n",
        "    return output, q_scale, zero_point, minVal\n",
        "\n",
        "  def quantizeTensorBits(self, input_tensor, numBits, isActivations=False,\n",
        "                         debug_mode=False):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        Input - Tensor which needs to be quantized\n",
        "      Returns:\n",
        "        Output - Quantized tensor with each value being a ```numBits``` wide bits\n",
        "    \"\"\"\n",
        "    # This function is used to quantize the input tensor\n",
        "    signQuantizedTensor, scale, zero_point, minVal = self.quantizeTensor(input_tensor, numBits, isActivations)\n",
        "    # assert len(signQuantizedTensor.shape) == 4, \"quantizeTensorBits: Tensor has wrong length\"\n",
        "    if isActivations:\n",
        "      assert len(input_tensor.shape) == 3, \"quantizeTensorBits: activations tensor has wrong shape\"\n",
        "    else:\n",
        "      assert len(input_tensor.shape) == 4, \"quantizeTensorBits: weights tensor has wrong shape\"\n",
        "    # \n",
        "    if input_tensor.requires_grad:\n",
        "      input_np = input_tensor.detach().numpy()\n",
        "    else:\n",
        "      input_np = input_tensor.numpy()\n",
        "    \n",
        "    output = np.zeros((signQuantizedTensor.shape[0], signQuantizedTensor.shape[1], signQuantizedTensor.shape[2], signQuantizedTensor.shape[3], numBits))\n",
        "    # \n",
        "    for i in range(input_np.shape[0]):\n",
        "      for j in range(input_np.shape[1]):\n",
        "        for k in range(input_np.shape[2]):\n",
        "          for l in range(input_np.shape[3]):\n",
        "            output[i, j, k, l, :] = self.generateTwosComplement(input_np[i, j, k, l], numBits)\n",
        "    \n",
        "    input_tensor = torch.from_numpy(input_np)\n",
        "    return torch.tensor(output), scale, zero_point, minVal\n",
        "  \n",
        "  def generateTwosComplement(self, value, numBits, debug_mode=False):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        value - Input number for which we need to generate two's complement\n",
        "        numBits - Number of bits in the input value\n",
        "      Returns:\n",
        "        output - Two's complement representation for the input num\n",
        "    \"\"\"\n",
        "    bins = [2**i for i in range(0, numBits)]\n",
        "    bitMask = (2 ** numBits) - 1\n",
        "    divisor = 2 ** (numBits-1)\n",
        "    value = int(value)\n",
        "    sign = (value & bitMask) >> (numBits - 1)\n",
        "    if debug_mode:\n",
        "      print(\"Bins generated are {}\".format(bins))\n",
        "      print(\"Sign is {}\".format(sign))\n",
        "\n",
        "    result = [0] * numBits\n",
        "    left, right = 0, numBits - 1\n",
        "\n",
        "    # sign bit \n",
        "    if sign:\n",
        "      value = (1 << (numBits-1)) + value\n",
        "\n",
        "    # Finding the binary representation\n",
        "    while value > 0 and left <= right:\n",
        "      # START\n",
        "      if debug_mode:\n",
        "        print(\"START: Left = {}, right = {}, value = {}\".format(left, right, value))\n",
        "\n",
        "      mid = left + int((right - left)/2)\n",
        "      # The first case arises only when 1 is left and mid = 0\n",
        "      if value == bins[mid]:\n",
        "        result[mid] = 1\n",
        "        value = 0\n",
        "        break\n",
        "      elif value <= bins[mid] and mid >= 1 and value > bins[mid-1]:\n",
        "        result[mid-1] = 1\n",
        "        value -= bins[mid-1]\n",
        "        left, right = 0, numBits - 1\n",
        "        if value == 0:\n",
        "          break\n",
        "      elif bins[mid] > value:\n",
        "        right = mid-1\n",
        "      else:\n",
        "        left = mid + 1\n",
        "\n",
        "      # STOP\n",
        "      if debug_mode:\n",
        "        print(\"STOP: Left = {}, right = {}, value = {}\".format(left, right, value))\n",
        "\n",
        "    if debug_mode:\n",
        "      print(\"Result for input = {} is {}\".format(value, result))\n",
        "\n",
        "    if sign:\n",
        "      result[-1] = 1\n",
        "    # Reverse the list and return it  \n",
        "    result.reverse()\n",
        "    return result\n",
        "\n",
        "  def print_size_of_model(self, model, filename=\"defaultModel.p\"):\n",
        "    \"\"\"\n",
        "      Function used to print the size of the model\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    print(\"Size (MB): \", os.path.getsize(filename)/1e6)\n",
        "  \n",
        "  def quantizeActivations(self, inputs):\n",
        "    \"\"\"\n",
        "      Function to quantize the input activations and return the value accordingly\n",
        "    \"\"\"\n",
        "    self.model_fp32.qconfig = torch.quantization.default_qconfig\n",
        "    x = self.quantizeTensorBits(inputs, numBits)\n",
        "    return x\n",
        "\n",
        "  def quantizeModel(self):\n",
        "    \"\"\"\n",
        "      Takes the input tensor provided and quantizes it to a static INT8 representation\n",
        "      While returning, it returns the bit level representation of each value in a string\n",
        "    \"\"\"\n",
        "    self.model_fp32.qconfig = torch.quantization.default_qconfig\n",
        "    print(\"Quantization configuration is {}\".format(self.model_fp32.qconfig))\n",
        "    print(\"Model size prior to Quantization is \")\n",
        "    model_size_pre_quantize = self.print_size_of_model(self.model_fp32, \"defaultVGG16.p\")\n",
        "    torch.quantization.prepare(self.model_fp32, inplace=True)\n",
        "    print(\"Post training quantization preparation step\")\n",
        "    self.model_fp32.eval()\n",
        "    # Conversion from FP32 to Fixed point representation\n",
        "    torch.quantization.convert(self.model_fp32, inplace=True)\n",
        "    print(\"Model size post Quantization is \")\n",
        "    model_size_post_quantize = self.print_size_of_model(self.model_fp32, \"quantizedVGG16.p\")\n",
        "    # Print statements after quantizing the model\n",
        "    print(\"quantized the model as per the default quantization configuration\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCYRtCYQpJ-"
      },
      "source": [
        "### This cell has the code for the ```Bit-Serial``` product computation of two tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsLiIPt3mwmh"
      },
      "source": [
        "import math\n",
        "\n",
        "class SerialInnerProduct:\n",
        "  \"\"\"\n",
        "    This class models the behavior of a Serial Inner Product Unit.\n",
        "\n",
        "  \"\"\"\n",
        "  # Default constructor\n",
        "  def __init__(self, inputs, weights, kernel_size_x, \\\n",
        "               kernel_size_y, numBits, quantizer, \\\n",
        "               stride_x=1, stride_y=1, padding_x=0, padding_y=0):\n",
        "    \"\"\"\n",
        "      Args: inputs  - Quantized inputs\n",
        "            weights - Quantized weights\n",
        "            integer_bits - Number of Integer bits used for representation\n",
        "            frac_bits - Number of Fractional bits used for representation\n",
        "            kernel_size_x - Filter size (Fx)\n",
        "            kernel_size_y - Filter size (Fy)\n",
        "      Returns:\n",
        "            None\n",
        "    \"\"\"\n",
        "    self._activations = inputs  # Input = 16 pixels * 8 bits per pixel\n",
        "    self._weights = weights     # Weight = 16 pixels * 1 bit per pixel\n",
        "    # Configuration for 16 x 16 SIP unit\n",
        "    self._num_input_lanes = 16  # Convert this to a knob\n",
        "    self._num_weight_lanes = 16 # Convert this to a knob\n",
        "    # Kernel sizes have been mentioned here\n",
        "    self._kernel_size_x = kernel_size_x # Compute the kernel_size_x\n",
        "    self._kernel_size_y = kernel_size_y # Compute the kernel_size_y\n",
        "    # Store the x and y strides\n",
        "    self._stride_x = stride_x\n",
        "    self._stride_y = stride_y\n",
        "    # Store the x and y padding\n",
        "    self._pad_x = padding_x\n",
        "    self._pad_y = padding_y\n",
        "    # If the padding differs, we need to pad the given inputs\n",
        "    if (self._pad_x > 0 or self._pad_y > 0):\n",
        "      self._activations = self.padInputs()\n",
        "    # \n",
        "    self.numBits = numBits\n",
        "    self.qzer = quantizer\n",
        "\n",
        "  # Function to pad the inputs based on the parameters specified in the constructor\n",
        "  def padInputs(self):\n",
        "    \"\"\"\n",
        "      Args: None  \n",
        "      Returns: The padded numpy input array\n",
        "    \"\"\"\n",
        "    # print(\"padding x = {}, y = {}\".format(self._pad_x, self._pad_y))\n",
        "    new_array = np.zeros((self._activations.shape[0], \\\n",
        "                          (self._activations.shape[1] + (2*self._pad_x)),\\\n",
        "                          (self._activations.shape[2] + (2*self._pad_y))))\n",
        "    # print(\"Shape of activations = {}\".format(self._activations.shape))\n",
        "    # print(\"Shape of new_array = {}\".format(new_array.shape))\n",
        "    # Create a new array\n",
        "    new_array[:, 1:self._activations.shape[1]+1, 1:self._activations.shape[2]+1] = self._activations\n",
        "    return new_array\n",
        "\n",
        "  # Process the given window with the activations and weights\n",
        "  def processWindow(self, activations, weights, bitNumber, debug_mode=False):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "          Activations - Input activation for processing the given input\n",
        "          Weights - Entire weight values for the given filters\n",
        "          bitNumber - Choose which bitnumber needs to be chosen from the weight\n",
        "          debug_mode - Print whether the debug_info needs to be printed or not\n",
        "      Returns:\n",
        "          Output - number of channels * 8 bit for each result\n",
        "    \"\"\"\n",
        "    # assert activations.shape == weights.shape # This doesn't hold after the conversion of values to 8 bit quantized numbers\n",
        "    result = np.zeros(activations.shape)\n",
        "    #print(\"processWindow: activations = {}, weights = {}\".format(activations, weights))\n",
        "    for i in range(len(activations)):\n",
        "      # Extract the bit from the given bitNumber\n",
        "      bit = int(weights[i][bitNumber])\n",
        "      # bitNumber == 0 represents the most significant bit\n",
        "      if bitNumber == 0:\n",
        "        if bit == 1:\n",
        "          result[i] = -1 * activations[i]\n",
        "        else:\n",
        "          assert bit == 0, \"bit neither 1 nor 0 = \" + str(bit)\n",
        "          result[i] = 0\n",
        "      else:\n",
        "        if bit == 1:\n",
        "          result[i] = activations[i]\n",
        "        else:\n",
        "          assert bit == 0, \"bit neither 1 nor 0 = \" + str(bit)\n",
        "          result[i] = 0\n",
        "    if debug_mode:\n",
        "      print(\"Result is {}\".format(result)) \n",
        "    return result\n",
        "\n",
        "  # This is used to process a bulk of 16 x 16 tiles to generate the given output\n",
        "  def processTile(self, inputs, weights, bitNumber, debug_mode=False):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "          inputs - Given input activation lanes\n",
        "          weights - Weights of the given neural network\n",
        "          bitNumber - The exact bit number which needs to be processed\n",
        "      Returns:\n",
        "          The outputs of each of the given lanes\n",
        "    \"\"\"\n",
        "    if debug_mode:\n",
        "      print(\"processTile: inputs shape = {} and weights shape = {}\".format(inputs.shape, weights.shape))\n",
        "    partialOutputs = np.zeros((weights.shape[0], weights.shape[1]))\n",
        "    for filter in np.arange(0, weights.shape[0]):\n",
        "      weight_lanes = weights[filter][:]\n",
        "      if debug_mode:\n",
        "        print(\"processTile: weight_lanes shape = {} and weight_lanes value = {}\".format(weight_lanes.shape, weight_lanes))\n",
        "      partialOutputs[filter][:] = self.processWindow(inputs, weight_lanes, bitNumber, False)\n",
        "    if debug_mode:\n",
        "      print(\"Shape of partialOutputs = {}\".format(partialOutputs.shape))\n",
        "    return partialOutputs\n",
        "\n",
        "  # TODO: We need to model implementation of ReLU, MaxPooling of every layer\n",
        "  # TO THINK: Whether we could insert ways to process these layers in the same hardware\n",
        "  def processInputWeights(self, inputs, weights, layerNumber, pixelsPerWindow, debug_mode=False):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        inputs - The quantized input numpy array\n",
        "        weights - The quantized weights numpy array\n",
        "        layerNumber - Extract the weights for the corresponding layerNumber\n",
        "        bitNumber - Extract the corresponding bitNumber's result\n",
        "        pixelsPerWindow - Number of pixels in each window (Number of cycles required to compute this number)\n",
        "      Returns:\n",
        "        The matrix multiplication of the given tensors\n",
        "    \"\"\"\n",
        "    numFilters = 0\n",
        "    self._activations = inputs\n",
        "    # \n",
        "    if self._pad_x > 0 or self._pad_y > 0:\n",
        "      inputs = self.padInputs()\n",
        "    numFilters = weights.shape[0]\n",
        "    inputChannels, inputSize_x, inputSize_y = inputs.shape[0], inputs.shape[1], inputs.shape[2]\n",
        "    outputSize_x = int((inputSize_x - self._kernel_size_x)/self._stride_x) + 1\n",
        "    outputSize_y = int((inputSize_y - self._kernel_size_y)/self._stride_y) + 1\n",
        "    # quantize the weights initially\n",
        "    weight, q_scale, zero_point, minVal = self.qzer.quantizeTensorBits(weights, self.numBits, False)\n",
        "    print(\"q_scale {}, zero_point {}, minVal {} for weights tensor\".format(q_scale, zero_point, minVal))\n",
        "    if debug_mode:\n",
        "      print(\"Input channels = {}, input rows = {}, input cols = {}\".format(inputChannels, inputSize_x, inputSize_y))\n",
        "      print(\"Input shape = {}\".format(inputs.shape))\n",
        "    outputImages = np.zeros((numFilters, outputSize_x, outputSize_y), dtype=float)\n",
        "    outputImageDict = {}\n",
        "    expectedImages = np.zeros((numFilters, outputSize_x, outputSize_y), dtype=float)\n",
        "    fileName = \"/content/gdrive/MyDrive/goldenOutputsv2/outputConvLayer\"+str(layerNumber + 1)+\".npy\"\n",
        "    expectedImages = np.load(fileName)\n",
        "    # \n",
        "    for bitNumber in np.arange(0, self.numBits):\n",
        "      for r in np.arange(0, inputSize_x, self._stride_x):\n",
        "        for c in np.arange(0, inputSize_y, self._stride_y):\n",
        "          for kernel_row in np.arange(0, self._kernel_size_x):\n",
        "            for kernel_col in np.arange(0, self._kernel_size_y):\n",
        "              for window in np.arange(0, math.ceil(inputChannels/pixelsPerWindow)):\n",
        "                windowStartPoint, windowEndPoint = window * pixelsPerWindow, min((window + 1) * pixelsPerWindow, inputChannels)\n",
        "                for filter in np.arange(0, math.ceil(numFilters/self._num_weight_lanes)):\n",
        "                  filterStartPoint, filterEndPoint = filter * self._num_weight_lanes, min((filter + 1) * self._num_weight_lanes, numFilters)\n",
        "                  output_r, output_c = int((r - kernel_row)/self._stride_x), int((c - kernel_col)/self._stride_y)\n",
        "                  # Avoid processing out of bounds problems in this code\n",
        "                  if (r < kernel_row or c < kernel_col or output_r >= outputSize_x or output_c >= outputSize_y):\n",
        "                    continue\n",
        "                  for f in np.arange(filterStartPoint, filterEndPoint):\n",
        "                    for pixel in np.arange(windowStartPoint, windowEndPoint):\n",
        "                      bit = weight[f][pixel][kernel_row][kernel_col][bitNumber]\n",
        "                      key = str(f) + str(output_r) + str(output_c)\n",
        "                      if key not in outputImageDict:\n",
        "                        outputImages[f][output_r][output_c] = (minVal + (128 * q_scale)) * inputs[pixel, r, c]\n",
        "                        outputImageDict[key] = True\n",
        "                      if bit == 1:\n",
        "                        if bitNumber == 0:\n",
        "                          outputImages[f][output_r][output_c] += (-128 * q_scale * inputs[pixel, r, c])\n",
        "                        else:\n",
        "                          outputImages[f][output_r][output_c] += ((2 ** (7-bitNumber)) * q_scale * inputs[pixel, r, c])\n",
        "\n",
        "                    #print(\"Computed result at bitNumber = {} is {}\".format(7-bitNumber, outputImages[f,output_r,output_c]))\n",
        "                    #print(\"Expected result is {}\".format(expectedImages[0,f,output_r,output_c]))\n",
        "    print(\"Expected output is {}\".format(expectedImages[0]))\n",
        "    print(\"Actual output is {}\".format(outputImages))\n",
        "    return outputImages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZkK_kMpRcxE"
      },
      "source": [
        "### Wrapper class for a Convolutional layer and VGG16 network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p28SLZhxr45S"
      },
      "source": [
        "class ConvLayer(nn.Module):\n",
        "\t\"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "\tdef __init__(self, in_channels, out_channels, kernel_size, padding = 0):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.in_size, self.out_size, self.kernel_size = in_channels, out_channels, kernel_size\n",
        "\t\tself.conv = nn.Conv2d(self.in_size, self.out_size, kernel_size=self.kernel_size, padding = padding)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.conv(x)\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(VGG16, self).__init__()\n",
        "\t\t\n",
        "\t\tself.conv1_1 = ConvLayer(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "\t\tself.layer1_1 = nn.Sequential(nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
        "\t\tself.conv1_2 = ConvLayer(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "\t\tself.layer1_2 = nn.Sequential(nn.BatchNorm2d(64),nn.ReLU(inplace=True))\n",
        "\t\tself.layer1_2p =nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\t\tself.conv2_1 = ConvLayer(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "\t\tself.layer2_1 = nn.Sequential(nn.BatchNorm2d(128), nn.ReLU(inplace=True))  \n",
        "\t\tself.conv2_2 = ConvLayer(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "\t\tself.layer2_2 = nn.Sequential(nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
        "\t\tself.layer2_2p =nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\t\t\n",
        "\t\tself.conv3_1 = ConvLayer(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "\t\tself.layer3_1 = nn.Sequential(nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
        "\t\tself.conv3_2 = ConvLayer(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "\t\tself.layer3_2 = nn.Sequential(nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
        "\t\tself.conv3_3 = ConvLayer(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "\t\tself.layer3_3 = nn.Sequential(nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
        "\t\tself.layer3_3p =nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\t\t\n",
        "\t\tself.conv4_1 = ConvLayer(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "\t\tself.layer4_1 = nn.Sequential(nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
        "\t\tself.conv4_2 = ConvLayer(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\t\tself.layer4_2 = nn.Sequential(nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
        "\t\tself.conv4_3 = ConvLayer(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\t\tself.layer4_3 = nn.Sequential(nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
        "\t\tself.layer4_3p =nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\t\t\n",
        "\t\tself.conv5_1 = ConvLayer(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\t\tself.layer5_1 = nn.Sequential(nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
        "\t\tself.conv5_2 = ConvLayer(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\t\tself.layer5_2 = nn.Sequential(nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
        "\t\tself.conv5_3 = ConvLayer(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\t\tself.layer5_3 = nn.Sequential(nn.BatchNorm2d(512), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\t\t\n",
        "\n",
        "\t\tself.fc1 = nn.Linear(512, 512)\n",
        "\t\tself.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t#print('Input = ', x.shape)\n",
        "\t\tout = self.conv1_1(x)\n",
        "\t\t#print('Conv11 = ',out.shape)\n",
        "\t\tout = self.layer1_1(out)\n",
        "\t\t#print('Layer11 = ',out.shape)\n",
        "\t\tout = self.conv1_2(out)\n",
        "\t\t#print('Conv12 = ',out.shape)\n",
        "\t\tout = self.layer1_2(out)\n",
        "\t\tout = self.layer1_2p(out)\n",
        "\t\t#print('Layer 12 = ',out.shape)\n",
        "\t\t\n",
        "\t\tout = self.conv2_1(out)\n",
        "\t\tout = self.layer2_1(out)\n",
        "\t\tout = self.conv2_2(out)\n",
        "\t\tout = self.layer2_2(out)\n",
        "\t\tout = self.layer2_2p(out)\n",
        "\t\n",
        "\t\tout = self.conv3_1(out)\n",
        "\t\tout = self.layer3_1(out)\n",
        "\t\tout = self.conv3_2(out)\n",
        "\t\tout = self.layer3_2(out)\n",
        "\t\tout = self.conv3_3(out)\n",
        "\t\tout = self.layer3_3(out)\n",
        "\t\tout = self.layer3_3p(out)\n",
        "\t\t\n",
        "\t\tout = self.conv4_1(out)\n",
        "\t\tout = self.layer4_1(out)\n",
        "\t\tout = self.conv4_2(out)\n",
        "\t\tout = self.layer4_2(out)\n",
        "\t\tout = self.conv4_3(out)\n",
        "\t\tout = self.layer4_3(out)\n",
        "\t\tout = self.layer4_3p(out)\n",
        "\t\t\n",
        "\t\tout = self.conv5_1(out)\n",
        "\t\tout = self.layer5_1(out)\n",
        "\t\tout = self.conv5_2(out)\n",
        "\t\tout = self.layer5_2(out)\n",
        "\t\tout = self.conv5_3(out)\n",
        "\t\tout = self.layer5_3(out)\n",
        "\t\t\n",
        "\t\tout = out.view(out.size(0), -1)\n",
        "\t\tout = self.fc1(out)\n",
        "\t\tout = self.fc2(out)\n",
        "\t\treturn out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZZ7VtM7TO8k"
      },
      "source": [
        "### Testing Quantization framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCbavGHdhf8n",
        "outputId": "2b097990-0ba8-481c-dcc4-d28a9a4b187e"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# For Quantization purposes\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torchvision.models as models\n",
        "\n",
        "# import numpy as np\n",
        "import numpy as np\n",
        "from time import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn import cluster\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "import pickle\n",
        "torch.set_printoptions(linewidth=120)\n",
        "torch.set_grad_enabled(True)\t # On by default, leave it here for clarity\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import struct\n",
        "\n",
        "n_patterns = 32\n",
        "n_groups = 8\n",
        "Lambda_rate = 1.2\n",
        "eval_pattern = 0\n",
        "# Customized information\n",
        "debug_mode = True\n",
        "integer_bits = 4\n",
        "frac_bits = 4\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "\ttransforms.RandomCrop(32, padding=4),\n",
        "\ttransforms.RandomHorizontalFlip(),\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "print(os.getcwd(), os.listdir())\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data/', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False, num_workers=2)\n",
        "import os\n",
        "print(\"CWD is {}\".format(os.getcwd()))\n",
        "print(os.listdir())\n",
        "\n",
        "\n",
        "# Obtaining the FP32 version of the model\n",
        "import torchvision.models as models\n",
        "#model_vgg16 = models.vgg16(pretrained=True)\n",
        "model_vgg16 = torch.load('/content/gdrive/MyDrive/model_vgg16_cifar10_bline.torch', map_location=torch.device('cpu'))\n",
        "\n",
        "# Quantizing the activations for this input model\n",
        "activations = []\n",
        "for data in test_loader:\n",
        "  inputs, labels = data\n",
        "  activations.append(inputs)\n",
        "activations = torch.tensor(np.concatenate(activations, axis=0))\n",
        "if debug_mode:\n",
        "  print(\"activations type = {}, shape = {}\".format(type(activations), activations.shape))\n",
        "\n",
        "qzer = Quantizer(model_vgg16, activations, integer_bits+frac_bits, debug_mode)\n",
        "# Quantizing the given model and obtaining the quantized version of the model\n",
        "#qzer.quantizeModel()\n",
        "# TODO: This is probably not required\n",
        "# qzer.quantizeActivations(activations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content ['.config', 'gdrive', 'data', 'sample_data']\n",
            "Files already downloaded and verified\n",
            "CWD is /content\n",
            "['.config', 'gdrive', 'data', 'sample_data']\n",
            "activations type = <class 'torch.Tensor'>, shape = torch.Size([10000, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTO3N9saTSw7"
      },
      "source": [
        "### Activations are computed here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lj5P2oihrTb",
        "outputId": "a74c0f62-7bef-4b2e-a45c-1994b872726b"
      },
      "source": [
        "input_data = activations[0]\n",
        "print(\"Input Shape = {}\".format(input_data.shape))\n",
        "print(\"Shape of activations = {}\".format(activations.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape = torch.Size([3, 32, 32])\n",
            "Shape of activations = torch.Size([10000, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZOchu7beMA"
      },
      "source": [
        "# convert(inputTensor)\n",
        "def convert(inputTensor):\n",
        "  \"\"\"\n",
        "    Args: inputTensor - Convert this tensor from Qint8 to float32\n",
        "    Returns: Converted output tensor\n",
        "  \"\"\"\n",
        "  assert len(inputTensor.shape) == 4\n",
        "  shape = inputTensor.shape\n",
        "  outputTensor = torch.tensor(np.zeros(inputTensor.shape)) \n",
        "  for i in range(0, shape[0]):\n",
        "    for j in range(0, shape[1]):\n",
        "      for k in range(0, shape[2]):\n",
        "        for l in range(0, shape[3]):\n",
        "          outputTensor[i,j,k,l] = float(inputTensor[i,j,k,l].item())\n",
        "  return outputTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEIhRlNdX4oz"
      },
      "source": [
        "### Code to obtain the weights of the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Ue4cWnkIvF",
        "outputId": "e2682b84-d439-4fcc-8654-ccac2a1dccab"
      },
      "source": [
        "nonquantized_model = torch.load('/content/gdrive/MyDrive/model_vgg16_cifar10_bline.torch', map_location=torch.device('cpu'))\n",
        "#quantizedModel = torch.load('./quantizedVGG16.p',map_location=torch.device('cpu'))\n",
        "#print(\"Type of quantized model = {}\".format(type(quantizedModel)))\n",
        "print(\"Type of non quantized model = {}\".format(type(nonquantized_model)))\n",
        "#print(len(quantizedModel.keys()), quantizedModel.keys())\n",
        "#print(quantizedModel['features.0.bias'], quantizedModel['features.0.weight'])\n",
        "#weight_layer1 = quantizedModel['conv1_1.conv.weight']\n",
        "#outputChannels = weight_layer1.shape[0]\n",
        "#inputChannels = weight_layer1.shape[1]\n",
        "#kernel_rows = weight_layer1.shape[2]\n",
        "#kernel_cols = weight_layer1.shape[3]\n",
        "numBits = 8\n",
        "\n",
        "# Evaluate Quantization per Tensor\n",
        "# PyTorch Quantization Per Tensor\n",
        "#print(quantizedModel.keys())\n",
        "# Biases, Weights, Scales\n",
        "biases = []\n",
        "scales = []\n",
        "zero_points = []\n",
        "# \n",
        "values = []\n",
        "nonquantized_weights = []\n",
        "weights = []\n",
        "convLayerNames = []\n",
        "batchNormLayerNames = []\n",
        "batchNormWeights = []\n",
        "\n",
        "# Set this to TRUE if preTrained model is used for quantizing\n",
        "preTrainedModel = False\n",
        "quantized = False\n",
        "if quantized:\n",
        "  # Understood quantization currently\n",
        "  for k, v in quantizedModel.items():\n",
        "    if preTrainedModel: \n",
        "      if \"features\" in k and \"weight\" in k:\n",
        "        print(\"WEIGHT: key = {}, Type(value) = {}, value.shape = {}\".format(k, type(v), v.shape))\n",
        "        values.append(v)\n",
        "        weights.append(convert(v))\n",
        "        print(\"SCALES: key = {}, scale = {}\".format(k, v.q_scale()))\n",
        "        scales.append(v.q_scale())\n",
        "      elif \"features\" in k and \"bias\" in k:\n",
        "        print(\"BIAS: key = {}, Type(value) = {}, value.shape = {}\".format(k, type(v), v.shape))\n",
        "        biases.append(v)\n",
        "      elif \"features\" in k and \"zero_point\" in k:\n",
        "        print(\"ZERO_POINT: type = {}\".format(type(v)))\n",
        "        print(\"ZERO_POINT: \", v.item())\n",
        "        zero_points.append(v.item())\n",
        "    else:\n",
        "      if \"conv\" in k:\n",
        "        if \"weight\" in k:\n",
        "          print(\"WEIGHT: key = {}, Type(value) = {}, value.shape = {}\".format(k, type(v), v.shape))\n",
        "          nonquantized_weights.append(v)\n",
        "          weights.append(convert(v))\n",
        "          print(\"SCALES: key = {}, scale = {}\".format(k, v.q_scale()))\n",
        "          scales.append(v.q_scale())\n",
        "          #print(\"ZERO_POINT: key = {}, zero_point = {}\".format(k, v))\n",
        "        elif \"bias\" in k:\n",
        "          print(\"BIAS: key = {}, Type(value) = {}, value.shape = {}\".format(k , type(v), v.shape))\n",
        "        elif \"zero_point\" in k:\n",
        "          # v is a tensor\n",
        "          zero_points.append(v.item())\n",
        "          print(\"ZERO_POINT: \", v.item())\n",
        "else:\n",
        "  # Go through every layer and get the weights of the model\n",
        "  for name, param in nonquantized_model.named_parameters():\n",
        "    if \"conv\" in name:\n",
        "      if \"weight\" in name:\n",
        "        convLayerNames.append(name)\n",
        "        weights.append(param)\n",
        "        nonquantized_weights.append(param)\n",
        "        #print(\"NONQUANTIZED: CONV LAYER {}, WEIGHTS SHAPE {}\".format(name, param.shape))\n",
        "      elif \"bias\" in name:\n",
        "        biases.append(param)\n",
        "        #print(\"NONQUANTIZED: CONV LAYER {}, BIAS SHAPE {}\".format(name, param))\n",
        "    elif \"layer\" in name:\n",
        "      if \"weight\" in name:\n",
        "        batchNormLayerNames.append(name)\n",
        "        batchNormWeights.append(param)\n",
        "        #print(\"NONQUANTIZED: SEQUENTIAL LAYER {}, WEIGHTS SHAPE {}\".format(name, param.shape))\n",
        "      elif \"bias\" in name:\n",
        "        biases.append(param)\n",
        "        #print(\"NONQUANTIZED: SEQUENTIAL LAYER {}, BIAS SHAPE {}\".format(name, param))\n",
        "\n",
        "# Measurements have been made here\n",
        "if quantized:\n",
        "  print(\"Length of quantized weights = {}\".format(len(weights)))\n",
        "else:\n",
        "  print(\"Length of nonquantized weights = {}\".format(len(nonquantized_weights)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of non quantized model = <class '__main__.VGG16'>\n",
            "Length of nonquantized weights = 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihtpftd9-YVu"
      },
      "source": [
        "weights = np.array(nonquantized_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7AuruqU7tCT",
        "outputId": "70bab1b0-d176-4286-b6d5-e4338a076018"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sfYHMoOwkem4",
        "outputId": "f4d1b8bc-7553-4c8c-ea1e-f5ddb5386166"
      },
      "source": [
        "# TODO: Set these as knobs and setup values for each one of them\n",
        "integer_bits, frac_bits = 4, 4\n",
        "stride_x, stride_y = 1, 1\n",
        "padding_x, padding_y = 1, 1\n",
        "kernel_size_x, kernel_size_y = 3, 3\n",
        "SIPTile = SerialInnerProduct(input_data, weights, \\\n",
        "                             kernel_size_x, kernel_size_y, \\\n",
        "                             integer_bits + frac_bits, qzer, \\\n",
        "                             stride_x, stride_y, \\\n",
        "                             padding_x, padding_y)\n",
        "# Read through the code to compute the layerNumber and weights for the given layer\n",
        "activationFilePath = \"/content/gdrive/MyDrive/goldenOutputs/convLayer\"\n",
        "weightFilePath = \"/content/gdrive/MyDrive/quantizedWeights/vgg16_model_quantized\"\n",
        "activations = []\n",
        "\n",
        "# Just pass the first output through the model\n",
        "for i in range(1, 14):\n",
        "  convLayer = np.load(activationFilePath + str(i) + \".npy\")\n",
        "  print(\"Shape of first image of convLayer = {} is {}\".format(i+1, convLayer[0,:,:,:].shape))\n",
        "  activations.append(convLayer[0,:,:,:])\n",
        "\n",
        "pixelsPerWindow = 16\n",
        "weightLen = len(weights)\n",
        "for layerNumber in range(0, len(nonquantized_weights)):\n",
        "  print(\"Continuing with the execution of layer number {}\".format(layerNumber))\n",
        "  weights = nonquantized_weights[layerNumber]\n",
        "  if weights.requires_grad:\n",
        "    weights = weights.detach()\n",
        "  inputs = torch.from_numpy(activations[layerNumber])\n",
        "  print(\"At layernumber = {}, type(inputs), type(weights) = {}, {}\".format(layerNumber, type(inputs), type(weights)))\n",
        "  print(\"At layernumber = {}, inputs.shape, weights.shape = {}, {}\".format(layerNumber, inputs.shape, weights.shape))\n",
        "  outputs = SIPTile.processInputWeights(inputs, weights, layerNumber, pixelsPerWindow, False)\n",
        "  np.save(\"/content/gdrive/MyDrive/quantizedOutputs/activationsLayer\"+str(layerNumber+1), outputs)\n",
        "  print(\"Proceeding with the execution of layer number {}\".format(layerNumber+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of first image of convLayer = 2 is (3, 32, 32)\n",
            "Shape of first image of convLayer = 3 is (64, 32, 32)\n",
            "Shape of first image of convLayer = 4 is (64, 16, 16)\n",
            "Shape of first image of convLayer = 5 is (128, 16, 16)\n",
            "Shape of first image of convLayer = 6 is (128, 8, 8)\n",
            "Shape of first image of convLayer = 7 is (256, 8, 8)\n",
            "Shape of first image of convLayer = 8 is (256, 8, 8)\n",
            "Shape of first image of convLayer = 9 is (256, 4, 4)\n",
            "Shape of first image of convLayer = 10 is (512, 4, 4)\n",
            "Shape of first image of convLayer = 11 is (512, 4, 4)\n",
            "Shape of first image of convLayer = 12 is (512, 2, 2)\n",
            "Shape of first image of convLayer = 13 is (512, 2, 2)\n",
            "Shape of first image of convLayer = 14 is (512, 2, 2)\n",
            "Continuing with the execution of layer number 0\n",
            "At layernumber = 0, type(inputs), type(weights) = <class 'torch.Tensor'>, <class 'torch.Tensor'>\n",
            "At layernumber = 0, inputs.shape, weights.shape = torch.Size([3, 32, 32]), torch.Size([64, 3, 3, 3])\n",
            "quantizeTensor: type(input_tensor) = <class 'torch.Tensor'>\n",
            "Types are <class 'float'> and <class 'float'>\n",
            "q_scale 0.007206607098672904, zero_point -112, minVal -0.8069930672645569 for weights tensor\n",
            "Expected output is [[[ 3.8503942e-01  6.9648349e-01  7.4173236e-01 ...  5.6735975e-01\n",
            "    5.2205580e-01  6.1489373e-01]\n",
            "  [ 5.2600592e-01  9.6367091e-01  1.0302963e+00 ...  9.6494311e-01\n",
            "    9.3103617e-01  9.9795967e-01]\n",
            "  [ 5.6353933e-01  1.0387803e+00  1.1491252e+00 ...  9.9630827e-01\n",
            "    9.6677065e-01  9.9820721e-01]\n",
            "  ...\n",
            "  [-8.4259176e-01 -1.0254664e+00 -7.3948252e-01 ... -1.0221094e+00\n",
            "   -6.2145036e-01  1.8274930e-01]\n",
            "  [-8.3871245e-01 -1.0060966e+00 -7.1815270e-01 ... -9.7685081e-01\n",
            "   -6.4049280e-01  1.5825400e-01]\n",
            "  [-3.9826158e-01 -3.7703153e-01 -1.7832932e-01 ... -1.8969323e-01\n",
            "    6.9885656e-02  4.0691388e-01]]\n",
            "\n",
            " [[ 1.0402268e-02  8.0887601e-03  1.8697072e-02 ... -7.6534465e-02\n",
            "   -1.0119633e-01 -1.0003201e-01]\n",
            "  [ 1.0022172e-02 -2.3676157e-04  1.3657668e-02 ... -9.7348630e-02\n",
            "   -1.2595583e-01 -1.2823860e-01]\n",
            "  [ 4.4719805e-03 -1.0314126e-02  3.1192622e-03 ... -8.8294402e-02\n",
            "   -1.1202022e-01 -1.1661438e-01]\n",
            "  ...\n",
            "  [-1.2688957e-01 -2.1199068e-01 -2.6859549e-01 ... -2.6457909e-01\n",
            "   -3.3799034e-01 -2.4303439e-01]\n",
            "  [-1.2528232e-01 -2.0608667e-01 -2.5894946e-01 ... -2.7304888e-01\n",
            "   -3.4373510e-01 -2.5579858e-01]\n",
            "  [-8.3388098e-02 -1.2641884e-01 -1.6201542e-01 ... -1.9254637e-01\n",
            "   -2.2694698e-01 -1.7253414e-01]]\n",
            "\n",
            " [[-1.5291120e-01 -1.7009798e-01 -2.1924651e-01 ...  8.2223728e-02\n",
            "    1.8082626e-01  4.6862075e-01]\n",
            "  [-4.8094198e-01 -6.1975461e-01 -6.5164471e-01 ... -3.9346480e-01\n",
            "   -2.8245491e-01  1.2628995e-01]\n",
            "  [-4.6441430e-01 -6.3121003e-01 -6.4387602e-01 ... -4.4748887e-01\n",
            "   -3.6418989e-01  7.1898095e-02]\n",
            "  ...\n",
            "  [ 6.5943879e-01  1.1617491e+00  1.4256152e+00 ...  1.2414447e+00\n",
            "    1.4553001e+00  1.2734201e+00]\n",
            "  [ 7.6997858e-01  1.1353393e+00  1.3456016e+00 ...  1.3989953e+00\n",
            "    1.3753366e+00  1.3513921e+00]\n",
            "  [ 6.4428300e-01  9.1093493e-01  9.8940068e-01 ...  1.1319416e+00\n",
            "    1.1082973e+00  9.4264549e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 7.0411325e-02  8.2063034e-02  9.8694645e-02 ... -3.3463389e-02\n",
            "   -7.8171030e-02 -1.2709099e-01]\n",
            "  [ 8.7227553e-02  1.0194346e-01  1.2310694e-01 ...  3.4577467e-03\n",
            "   -3.9204769e-02 -1.0841526e-01]\n",
            "  [ 8.2473271e-02  9.3504921e-02  1.1656423e-01 ...  1.1868446e-02\n",
            "   -2.0560268e-02 -9.1390461e-02]\n",
            "  ...\n",
            "  [-2.2560523e-01 -3.6445555e-01 -4.3532637e-01 ... -4.1572139e-01\n",
            "   -4.9822459e-01 -3.5833040e-01]\n",
            "  [-2.3294389e-01 -3.5956883e-01 -4.0834674e-01 ... -4.4107625e-01\n",
            "   -5.0210202e-01 -3.7756163e-01]\n",
            "  [-1.4063708e-01 -2.0150459e-01 -2.3096976e-01 ... -2.8729001e-01\n",
            "   -2.9295933e-01 -2.2967257e-01]]\n",
            "\n",
            " [[ 1.3605473e-03  2.6598640e-03  2.8540452e-03 ...  2.5973187e-03\n",
            "    2.5127616e-03  2.4997306e-03]\n",
            "  [ 1.6396113e-03  3.6235549e-03  3.9763884e-03 ...  3.6213601e-03\n",
            "    3.4678797e-03  3.5017296e-03]\n",
            "  [ 1.6943249e-03  3.7884910e-03  4.2509348e-03 ...  3.6126266e-03\n",
            "    3.4967384e-03  3.4226011e-03]\n",
            "  ...\n",
            "  [-2.8637315e-03 -3.7827697e-03 -2.7352595e-03 ... -3.9071008e-03\n",
            "   -1.9823697e-03  8.4090087e-04]\n",
            "  [-2.7115308e-03 -3.6157283e-03 -2.6882212e-03 ... -3.4122544e-03\n",
            "   -2.0125606e-03  5.2992423e-04]\n",
            "  [-1.6893342e-03 -2.4091341e-03 -1.9043336e-03 ... -2.2410520e-03\n",
            "   -1.5049601e-03 -1.0006171e-04]]\n",
            "\n",
            " [[ 2.6905799e-01  4.8982111e-01  4.9883762e-01 ...  4.1165122e-01\n",
            "    3.8623437e-01  4.3184650e-01]\n",
            "  [ 1.7324494e-01  4.0357566e-01  4.3551823e-01 ...  2.2336356e-01\n",
            "    1.8102932e-01  3.5530427e-01]\n",
            "  [ 1.8569574e-01  4.1910470e-01  4.7174653e-01 ...  2.1381436e-01\n",
            "    1.8480769e-01  3.5347891e-01]\n",
            "  ...\n",
            "  [-4.2754650e-01 -6.1796063e-01 -5.5499244e-01 ... -7.2960222e-01\n",
            "   -5.7196748e-01 -2.0904934e-01]\n",
            "  [-3.7123641e-01 -5.9521735e-01 -5.3500861e-01 ... -6.2564379e-01\n",
            "   -6.3572288e-01 -2.0345044e-01]\n",
            "  [-1.5153526e-01 -2.8704333e-01 -2.6710033e-01 ... -2.8334722e-01\n",
            "   -3.9572144e-01 -9.5442057e-02]]]\n",
            "Actual output is [[[ 0.83762959  1.31520707  1.26840806 ...  1.89866897  2.02741259\n",
            "    1.68175563]\n",
            "  [ 1.25620117  2.06207843  1.9981988  ...  2.9703168   3.17434713\n",
            "    2.56869366]\n",
            "  [ 1.37884109  2.27983055  2.23267682 ...  2.91243014  3.08212017\n",
            "    2.46771308]\n",
            "  ...\n",
            "  [-0.24082755  0.10766916  0.85307382 ...  0.77354491  1.75643833\n",
            "    1.84117413]\n",
            "  [-0.31107642  0.09714897  0.82110823 ...  0.82980025  1.86491124\n",
            "    1.8405983 ]\n",
            "  [-0.00981598  0.47456791  0.95533421 ...  0.9998114   1.72854557\n",
            "    1.48224056]]\n",
            "\n",
            " [[ 0.44203893  0.61676799  0.53690214 ...  1.25162036  1.4030079\n",
            "    0.96900558]\n",
            "  [ 0.71754019  1.08430549  0.96959139 ...  1.90187254  2.11311551\n",
            "    1.44453397]\n",
            "  [ 0.79472571  1.21715249  1.07365798 ...  1.78056845  1.99807672\n",
            "    1.35472017]\n",
            "  ...\n",
            "  [ 0.50340665  1.1048599   1.54133265 ...  1.71955264  2.26964514\n",
            "    1.65440525]\n",
            "  [ 0.42708007  1.04227868  1.48210135 ...  1.70276755  2.37905276\n",
            "    1.67089609]\n",
            "  [ 0.31989567  0.73664732  0.98516074 ...  1.15846456  1.66765066\n",
            "    1.11968269]]\n",
            "\n",
            " [[ 0.28381668  0.42993038  0.28826447 ...  1.39451333  1.66807578\n",
            "    1.52323792]\n",
            "  [ 0.22823126  0.45576038  0.29305384 ...  1.59011777  1.9405223\n",
            "    1.690786  ]\n",
            "  [ 0.32782312  0.58507978  0.41522857 ...  1.407106    1.73098614\n",
            "    1.53529142]\n",
            "  ...\n",
            "  [ 1.28186785  2.48777879  3.24309076 ...  3.23008826  4.06982859\n",
            "    3.19168064]\n",
            "  [ 1.31667895  2.38579722  3.09343363 ...  3.3785395   4.09603863\n",
            "    3.30105377]\n",
            "  [ 1.04069466  1.76839375  2.13274781 ...  2.47335765  2.99247391\n",
            "    2.24219848]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.51820166  0.70311798  0.62806867 ...  1.29573855  1.4241786\n",
            "    0.94435052]\n",
            "  [ 0.8100125   1.20435855  1.09750278 ...  2.00328823  2.19644119\n",
            "    1.46728573]\n",
            "  [ 0.88806421  1.27666841  1.14915368 ...  1.92232858  2.08742932\n",
            "    1.38380579]\n",
            "  ...\n",
            "  [ 0.36350884  0.74221036  1.13381089 ...  1.34530731  1.85229842\n",
            "    1.29530399]\n",
            "  [ 0.28527     0.71611551  1.10766226 ...  1.33359253  1.97196399\n",
            "    1.29790945]\n",
            "  [ 0.23810533  0.62885364  0.88339722 ...  0.87784624  1.34099737\n",
            "    0.83857668]]\n",
            "\n",
            " [[ 0.44462293  0.61300984  0.52091179 ...  1.32841032  1.50354274\n",
            "    1.06521328]\n",
            "  [ 0.72312312  1.09387908  0.96373866 ...  2.00065134  2.23870605\n",
            "    1.57010931]\n",
            "  [ 0.80636719  1.23573774  1.07880184 ...  1.91046699  2.11041947\n",
            "    1.46888346]\n",
            "  ...\n",
            "  [ 0.60652729  1.13633223  1.59590288 ...  1.79597472  2.38435396\n",
            "    1.66156414]\n",
            "  [ 0.53252395  1.10510535  1.54241199 ...  1.80730641  2.51026357\n",
            "    1.68621058]\n",
            "  [ 0.38736924  0.85019321  1.13405998 ...  1.18773917  1.65659035\n",
            "    1.07691278]]\n",
            "\n",
            " [[ 0.71371689  1.09959083  1.01729005 ...  1.73421809  1.88254167\n",
            "    1.48977056]\n",
            "  [ 0.89363617  1.49031508  1.39260269 ...  2.21628319  2.41139424\n",
            "    1.91442389]\n",
            "  [ 0.98988929  1.64817067  1.54400912 ...  2.11728073  2.28796979\n",
            "    1.81238263]\n",
            "  ...\n",
            "  [ 0.18206733  0.5183783   1.03660139 ...  1.0666091   1.8016187\n",
            "    1.43534943]\n",
            "  [ 0.16238092  0.51106161  1.00379991 ...  1.17796363  1.86746006\n",
            "    1.46484628]\n",
            "  [ 0.23939542  0.56928259  0.87122508 ...  0.90962641  1.26609424\n",
            "    0.97553368]]]\n",
            "Proceeding with the execution of layer number 1\n",
            "Continuing with the execution of layer number 1\n",
            "At layernumber = 1, type(inputs), type(weights) = <class 'torch.Tensor'>, <class 'torch.Tensor'>\n",
            "At layernumber = 1, inputs.shape, weights.shape = torch.Size([64, 32, 32]), torch.Size([64, 64, 3, 3])\n",
            "quantizeTensor: type(input_tensor) = <class 'torch.Tensor'>\n",
            "Types are <class 'float'> and <class 'float'>\n",
            "q_scale 0.0035513374150968067, zero_point -153, minVal -0.5424389839172363 for weights tensor\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c7f2a28bfa10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At layernumber = {}, type(inputs), type(weights) = {}, {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerNumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At layernumber = {}, inputs.shape, weights.shape = {}, {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerNumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIPTile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessInputWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayerNumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixelsPerWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/quantizedOutputs/activationsLayer\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerNumber\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Proceeding with the execution of layer number {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerNumber\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-91ad007677bd>\u001b[0m in \u001b[0;36mprocessInputWeights\u001b[0;34m(self, inputs, weights, layerNumber, pixelsPerWindow, debug_mode)\u001b[0m\n\u001b[1;32m    166\u001b[0m                           \u001b[0moutputImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_c\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m128\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpixel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                           \u001b[0moutputImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_c\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbitNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpixel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0;31m#print(\"Computed result at bitNumber = {} is {}\".format(7-bitNumber, outputImages[f,output_r,output_c]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n63xsNkub2cS",
        "outputId": "5c5a2b21-922a-412a-fa83-bf68cc943da7"
      },
      "source": [
        "quantized_output = np.load(\"/content/gdrive/MyDrive/quantizedOutputs/activationsLayer\" + str(1) + \".npy\")\n",
        "original_output = np.load(\"/content/gdrive/MyDrive/goldenOutputs/outputs_convLayer\" + str(1) + \".npy\")\n",
        "# The results are available here\n",
        "#print(scales[0], len(scales))\n",
        "print(\"Shape of quantized o/p = {} and original o/p = {}\".format(quantized_output.shape, original_output.shape))\n",
        "print(quantized_output.shape)\n",
        "minOutput, maxOutput = np.min(quantized_output), np.max(quantized_output)\n",
        "print(\"min and max of output are {} and {}\".format(minOutput, maxOutput))\n",
        "q_scale = (maxOutput - minOutput)/255\n",
        "zero_point = round(minOutput/q_scale)\n",
        "#print(\"q_scale of output tensor is {}\".format(q_scale))\n",
        "#print(\"zero_point is {}\".format(zero_point))\n",
        "#print(\"Quantized value is {}\".format(round((quantized_output[0][0][3]/q_scale) - zero_point)-128))\n",
        "print(quantized_output[0,0])\n",
        "print(original_output[0,0,0])\n",
        "#print(scales[0], zero_points[0])\n",
        "numNonZeroQuantized = [0]*quantized_output.shape[0]\n",
        "for idx in range(0, quantized_output.shape[0]):\n",
        "  numNonZeroQuantized[idx] = np.count_nonzero(quantized_output[idx,:,:])\n",
        "print(numNonZeroQuantized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of quantized o/p = (64, 32, 32) and original o/p = (100, 64, 32, 32)\n",
            "(64, 32, 32)\n",
            "min and max of output are -9.602738576316584 and 5.730517546559387\n",
            "[0.83762959 1.31520707 1.26840806 1.15626893 1.20936132 1.34911803\n",
            " 1.35902524 1.29573031 1.36324786 1.40598055 1.38623875 1.28771938\n",
            " 1.25243054 1.28503194 1.29528314 1.31506737 1.38608674 1.45427131\n",
            " 1.4578509  1.39166469 1.34686341 1.39870204 1.46430591 1.49781568\n",
            " 1.54173731 1.56475284 1.67353174 1.73785197 1.82485383 1.89866897\n",
            " 2.02741259 1.68175563]\n",
            "[0.38503942 0.6964835  0.74173236 0.68195057 0.6717471  0.759539\n",
            " 0.76300675 0.7356571  0.7247607  0.737562   0.73675555 0.71046275\n",
            " 0.7387419  0.7998264  0.81721026 0.8006569  0.77809197 0.76845104\n",
            " 0.7570511  0.69142395 0.6063319  0.5796229  0.5752282  0.6195668\n",
            " 0.623373   0.6172741  0.61819047 0.62541693 0.61613566 0.56735975\n",
            " 0.5220558  0.61489373]\n",
            "[1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBytaowODqEH"
      },
      "source": [
        "### This segment of the notebook has been used for testing the baseline implementation of the Bit-Serial architecture. Very simple tests have been used to verify the implementation of the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUCbllWNYxBT",
        "outputId": "29f325cf-5486-4980-a521-19d13c191dfb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "model = torch.load('/content/gdrive/MyDrive/model_vgg16_cifar10_bline.torch', map_location=torch.device('cpu'))\n",
        "inputMatrix = np.load('/content/gdrive/MyDrive/goldenOutputs/convLayer1.npy')\n",
        "print(\"Shape of Input matrix is {}\".format(inputMatrix.shape))\n",
        "inputTensor = torch.tensor(inputMatrix)\n",
        "qzer = Quantizer(nonquantized_model, inputTensor, integer_bits, frac_bits)\n",
        "\n",
        "convMatrix = nonquantized_weights[0].detach().numpy()\n",
        "print(\"Shape of weights = {}\".format(convMatrix.shape))\n",
        "org_convMatrix = np.copy(convMatrix)\n",
        "expectedResult = model.conv1_1(torch.from_numpy(inputMatrix))\n",
        "print(\"Expected result shape = {}\".format(expectedResult.shape))\n",
        "\n",
        "# Weights quantization\n",
        "maxVal = np.max(convMatrix)\n",
        "minVal = np.min(convMatrix)\n",
        "print(\"minVal and maxVal are {} and {}\".format(minVal, maxVal))\n",
        "q_scale = float((maxVal - minVal)/255)\n",
        "print(\"q_scale value is {}\".format(q_scale))\n",
        "zero_point = round(minVal/q_scale)\n",
        "print(\"Zero point is {}\".format(zero_point))\n",
        "for val in np.nditer(convMatrix, op_flags=['readwrite']):\n",
        "  val[...] = round(val/q_scale - zero_point) - 128\n",
        "\n",
        "# Test whether the values are between -128 and 127\n",
        "print(np.max(convMatrix), np.min(convMatrix))\n",
        "\n",
        "numFilters = convMatrix.shape[0]\n",
        "inputChannels = convMatrix.shape[1]\n",
        "numImages = inputMatrix.shape[0]\n",
        "\n",
        "assert inputChannels == inputMatrix.shape[1], \"Number of inputChannels is \" + str(inputChannels)\n",
        "numRows, numCols = inputMatrix.shape[2], inputMatrix.shape[3]\n",
        "kernelRows, kernelCols = convMatrix.shape[2], convMatrix.shape[3]\n",
        "\n",
        "# numBits, pixelsPerWindow, numWeightLanes, convMatrixQuantized\n",
        "numBits = 8\n",
        "pixelsPerWindow = 16\n",
        "numWeightLanes = 16\n",
        "convMatrixQuantized = np.zeros((numFilters, inputChannels, kernelRows, kernelCols, numBits), dtype=np.int8)\n",
        "print(convMatrix.shape)\n",
        "\n",
        "# This is the matrix after padding the inputs with zeros\n",
        "inputMatrixPadded = np.zeros((numImages, inputChannels, numRows+2, numCols+2), dtype=float)\n",
        "inputMatrixPadded[:,:,1:numRows+1,1:numCols+1] = inputMatrix\n",
        "\n",
        "# numRows, numCols\n",
        "numRows, numCols = inputMatrixPadded.shape[2], inputMatrixPadded.shape[3]\n",
        "\n",
        "print(convMatrix[0][0][0][0], convMatrixQuantized[0][0][0][0])\n",
        "# \n",
        "for i in range(convMatrix.shape[0]):\n",
        "  for j in range(convMatrix.shape[1]):\n",
        "    for k in range(convMatrix.shape[2]):\n",
        "      for l in range(convMatrix.shape[3]):\n",
        "        convMatrixQuantized[i][j][k][l][:] = qzer.generateTwosComplement(convMatrix[i,j,k,l], 8, False)\n",
        "\n",
        "print(convMatrix[0][0][0][0], convMatrixQuantized[0][0][0][0])\n",
        "# Assume strides are 1 and 1\n",
        "outputRows, outputCols = int(numRows - kernelRows) + 1, int(numCols - kernelCols) + 1\n",
        "result = np.zeros((numImages, numFilters, outputRows, outputCols), dtype=float)\n",
        "print(\"Shape of convMatrixQuantized = {}, result = {}\".format(convMatrixQuantized.shape, result.shape))\n",
        "resultDict = {}\n",
        "print(\"numBits = {}, numRows = {}, numCols = {}\".format(numBits, numRows, numCols))\n",
        "print(\"kernel rows = {}, kernel cols = {}, inputChannels = {}, numFilters = {}\".format(kernelRows, kernelCols, inputChannels, numFilters))\n",
        "print(\"pixelsPerWindow = {}, numWeightLanes = {}\".format(pixelsPerWindow, numWeightLanes))\n",
        "# This needs to be accelerated on the CPU\n",
        "for image in np.arange(0, numImages):\n",
        "  if image >= 1:\n",
        "    break\n",
        "  for bitNumber in np.arange(0, numBits):\n",
        "    for r in np.arange(0, numRows):\n",
        "      for c in np.arange(0, numCols):\n",
        "        for kr in np.arange(0, kernelRows):\n",
        "          for kc in np.arange(0, kernelCols):\n",
        "            for window in np.arange(0, math.ceil(inputChannels/pixelsPerWindow)):\n",
        "              windowStartPoint, windowEndPoint = window * pixelsPerWindow, min((window + 1) * pixelsPerWindow, inputChannels)\n",
        "              for filter in np.arange(0, math.ceil(numFilters/numWeightLanes)):\n",
        "                filterStartPoint, filterEndPoint = filter * numWeightLanes, min((filter + 1) * numWeightLanes, numFilters)\n",
        "                output_r, output_c = int(r - kr), int(c - kc)\n",
        "                if (r < kr or c < kc or output_r >= outputRows or output_c >= outputCols):\n",
        "                  continue\n",
        "                for f in np.arange(filterStartPoint, filterEndPoint):\n",
        "                  # Every value would have this term added to it\n",
        "                  for pixel in np.arange(windowStartPoint, windowEndPoint):\n",
        "                    key = str(f) + str(output_r) + str(output_c)\n",
        "                    if key not in resultDict:\n",
        "                      result[image, f, output_r, output_c] = (round(minVal/q_scale) + 128) * q_scale * inputMatrixPadded[image, pixel, r, c]\n",
        "                      resultDict[key] = True\n",
        "                    bit = convMatrixQuantized[filter][pixel][kr][kc][bitNumber]\n",
        "                    # This depends on the bit number and the value of the bit\n",
        "                    if bit == 1:\n",
        "                      if bitNumber == 0:\n",
        "                        result[image, f, output_r, output_c] +=  (-128 * inputMatrixPadded[image,pixel,r,c] * q_scale)\n",
        "                      else:\n",
        "                        result[image, f, output_r, output_c] += ((2**(7-bitNumber)) * inputMatrixPadded[image, pixel, r, c] * q_scale)\n",
        "    print(\"At bitnumber = {}, result matrix is {}\".format(7-bitNumber, result[0][0]))\n",
        "\n",
        "# Here, we print the expected result of the given matrix\n",
        "print(\"Expected result = {}\".format(expectedResult[0][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input matrix is (100, 3, 32, 32)\n",
            "Shape of weights = (64, 3, 3, 3)\n",
            "Expected result shape = torch.Size([100, 64, 32, 32])\n",
            "minVal and maxVal are -0.8069930672645569 and 1.0306917428970337\n",
            "q_scale value is 0.007206607332416609\n",
            "Zero point is -112\n",
            "127.0 -128.0\n",
            "(64, 3, 3, 3)\n",
            "-25.0 [0 0 0 0 0 0 0 0]\n",
            "-25.0 [1 1 1 0 0 1 1 1]\n",
            "Shape of convMatrixQuantized = (64, 3, 3, 3, 8), result = (100, 64, 32, 32)\n",
            "numBits = 8, numRows = 34, numCols = 34\n",
            "kernel rows = 3, kernel cols = 3, inputChannels = 3, numFilters = 64\n",
            "pixelsPerWindow = 16, numWeightLanes = 16\n",
            "At bitnumber = 7, result matrix is [[ 3.78128067  5.34399467  4.7686642  ... 10.04196974 11.28131521\n",
            "   8.52170656]\n",
            " [ 5.99114069  8.60637154  7.71051283 ... 15.30463829 17.01542574\n",
            "  12.55001618]\n",
            " [ 6.74676105  9.8347355   8.89968361 ... 14.76301908 16.22068092\n",
            "  11.75587846]\n",
            " ...\n",
            " [ 3.10746624  7.96715229 12.13065646 ... 13.12958593 17.59984337\n",
            "  15.02092809]\n",
            " [ 2.695035    7.68667464 11.50233736 ... 13.20490811 18.56974271\n",
            "  15.24943779]\n",
            " [ 3.09895403  6.80154593  9.07248012 ... 10.5252174  14.80877739\n",
            "  10.12434916]]\n",
            "At bitnumber = 6, result matrix is [[1.89064034 2.67199734 2.3843321  ... 5.02098487 5.6406576  4.26085328]\n",
            " [2.99557035 4.33972333 3.89291158 ... 7.67209278 8.52078092 6.27578258]\n",
            " [3.37338052 4.94719974 4.4785562  ... 7.40128317 8.12229092 5.87759613]\n",
            " ...\n",
            " [1.55373312 3.89941335 5.96440149 ... 6.47727738 8.69452456 7.38718539]\n",
            " [1.3475175  3.77929126 5.65806512 ... 6.52499683 9.18729741 7.49920506]\n",
            " [1.54947702 3.40077297 4.53624006 ... 5.18962187 7.2934036  4.95454228]]\n",
            "At bitnumber = 5, result matrix is [[1.23713328 1.9286235  1.7757923  ... 3.20210178 3.52093676 2.84453247]\n",
            " [1.7805997  2.82152055 2.62622804 ... 4.58792304 5.01456012 3.86626949]\n",
            " [1.98750198 3.16804543 2.98209833 ... 4.43452104 4.79170104 3.66155937]\n",
            " ...\n",
            " [0.49276974 1.42782981 2.62353177 ... 2.76290128 4.12360113 3.53753667]\n",
            " [0.43015558 1.42837915 2.50068324 ... 2.8868053  4.36881214 3.60030991]\n",
            " [0.56712971 1.33466114 2.02837498 ... 2.33607062 3.42195197 2.31886424]]\n",
            "At bitnumber = 4, result matrix is [[ 1.08114257  1.68849715  1.60527486 ...  2.57079069  2.77297305\n",
            "   2.30013409]\n",
            " [ 1.62121615  2.57796877  2.45892206 ...  3.87900625  4.1822418\n",
            "   3.34657741]\n",
            " [ 1.79435574  2.87486059  2.76506895 ...  3.78193925  4.03808768\n",
            "   3.19581891]\n",
            " ...\n",
            " [-0.07499027  0.55483144  1.54790667 ...  1.53622897  2.78247466\n",
            "   2.68600883]\n",
            " [-0.16467242  0.52800578  1.48553103 ...  1.59733438  2.93954519\n",
            "   2.72432857]\n",
            " [ 0.09781066  0.81815687  1.44450771 ...  1.60191444  2.54560961\n",
            "   2.11110538]]\n",
            "At bitnumber = 3, result matrix is [[ 0.90491363  1.39721189  1.33312055 ...  2.09637804  2.25144075\n",
            "   1.85074993]\n",
            " [ 1.37144425  2.19964808  2.10980874 ...  3.29389542  3.54327867\n",
            "   2.84603547]\n",
            " [ 1.5085588   2.45045646  2.37429564 ...  3.22497428  3.43512607\n",
            "   2.73236898]\n",
            " ...\n",
            " [-0.14019525  0.38748938  1.22416741 ...  1.15643638  2.27503725\n",
            "   2.23003191]\n",
            " [-0.2288917   0.35165781  1.17961008 ...  1.20599551  2.39316516\n",
            "   2.25007643]\n",
            " [ 0.05232974  0.65154719  1.20796228 ...  1.28105974  2.13395309\n",
            "   1.77375974]]\n",
            "At bitnumber = 2, result matrix is [[ 0.85544194  1.33645494  1.27688095 ...  1.98974983  2.13630866\n",
            "   1.7475147 ]\n",
            " [ 1.28554871  2.11222902  2.03418045 ...  3.10010315  3.32590401\n",
            "   2.67892292]\n",
            " [ 1.40807762  2.33551088  2.27899291 ...  3.03349304  3.22398397\n",
            "   2.57147635]\n",
            " ...\n",
            " [-0.16450279  0.25236034  1.03709616 ...  0.96106255  2.01065561\n",
            "   2.03285123]\n",
            " [-0.23813153  0.22599402  0.99814576 ...  1.0110974   2.12423371\n",
            "   2.03102221]\n",
            " [ 0.03012736  0.52924837  1.04338966 ...  1.09062794  1.87419585\n",
            "   1.57803656]]\n",
            "At bitnumber = 1, result matrix is [[ 8.52225632e-01  1.32552335e+00  1.27437197e+00 ...  1.92909369e+00\n",
            "   2.06275140e+00  1.70395250e+00]\n",
            " [ 1.27445888e+00  2.08374898e+00  2.01467594e+00 ...  3.01653039e+00\n",
            "   3.22717597e+00  2.60430179e+00]\n",
            " [ 1.39809138e+00  2.30391635e+00  2.25298500e+00 ...  2.95554438e+00\n",
            "   3.13171642e+00  2.50191329e+00]\n",
            " ...\n",
            " [-2.27919951e-01  1.53082696e-01  9.10783980e-01 ...  8.32255669e-01\n",
            "   1.83817656e+00  1.90503390e+00]\n",
            " [-2.96477074e-01  1.35924811e-01  8.76269190e-01 ...  8.90155678e-01\n",
            "   1.94330315e+00  1.90767213e+00]\n",
            " [-8.25743631e-04  4.91860175e-01  9.82600929e-01 ...  1.02725284e+00\n",
            "   1.77315186e+00  1.51518114e+00]]\n",
            "At bitnumber = 0, result matrix is [[ 0.83762962  1.31520711  1.2684081  ...  1.89866903  2.02741266\n",
            "   1.68175568]\n",
            " [ 1.25620121  2.06198538  1.9981029  ...  2.9702665   3.17431392\n",
            "   2.56869177]\n",
            " [ 1.37884114  2.2797546   2.23260372 ...  2.91237984  3.08208981\n",
            "   2.46771404]\n",
            " ...\n",
            " [-0.24082756  0.10788365  0.85333106 ...  0.77376797  1.75670699\n",
            "   1.84148836]\n",
            " [-0.31107643  0.09731219  0.82134553 ...  0.82999768  1.86515997\n",
            "   1.84091823]\n",
            " [-0.00981598  0.47456792  0.95533424 ...  0.99999744  1.72882847\n",
            "   1.48251491]]\n",
            "Expected result = tensor([[ 0.3850,  0.6965,  0.7417,  ...,  0.5674,  0.5221,  0.6149],\n",
            "        [ 0.5260,  0.9637,  1.0303,  ...,  0.9649,  0.9310,  0.9980],\n",
            "        [ 0.5635,  1.0388,  1.1491,  ...,  0.9963,  0.9668,  0.9982],\n",
            "        ...,\n",
            "        [-0.8426, -1.0255, -0.7395,  ..., -1.0221, -0.6215,  0.1827],\n",
            "        [-0.8387, -1.0061, -0.7182,  ..., -0.9769, -0.6405,  0.1583],\n",
            "        [-0.3983, -0.3770, -0.1783,  ..., -0.1897,  0.0699,  0.4069]], grad_fn=<SelectBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FycubXsVJhZv",
        "outputId": "9725184d-dbf7-4bce-fd53-84ada5200e73"
      },
      "source": [
        "#activationLayer1 = np.load('activationsLayer1.npy')\n",
        "#activationLayer2 = np.load('activationsLayer2.npy')\n",
        "outputLayer1 = np.load('/content/gdrive/MyDrive/goldenOutputs/outputs_convLayer1.npy')\n",
        "print(outputLayer1[0,:,:,:].shape)\n",
        "print(outputLayer1[0,0,0,1])\n",
        "nonZeroPerChannel = [0] * outputLayer1[0,:,:,:].shape[0]\n",
        "for idx in range(outputLayer1[0,:,:,:].shape[0]):\n",
        "  nonZeroPerChannel[idx] = np.count_nonzero(outputLayer1[0,idx,:,:])\n",
        "  #print(\"At channel {}, nonzero count is {}\".format(idx, nonZeroPerChannel[idx]))\n",
        "print(nonZeroPerChannel)\n",
        "#print(activationLayer1.shape)\n",
        "#print(activationLayer1[0][0][0])\n",
        "#print(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 32, 32)\n",
            "0.6964835\n",
            "[1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "22BpwhA2a-wF"
      },
      "source": [
        "print(inputLayer1[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oQ5gohUEuHcv"
      },
      "source": [
        "a = np.random.random((3, 3))\n",
        "b = np.ones((3, 3))\n",
        "c = np.multiply(a, b)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBJNpunUgHZq"
      },
      "source": [
        "filePath = \"/content/gdrive/MyDrive/quantizedOutputs/activationsLayer\"\n",
        "startLayer, endLayer = 3, 4\n",
        "for layerNumber in range(startLayer, endLayer):\n",
        "  fileName = filePath + str(layerNumber) + \".npy\"\n",
        "  npArray = np.load(fileName)\n",
        "  print(\"Shape of npArray at layerNumber = {} is {}\".format(layerNumber, npArray.shape))\n",
        "  if layerNumber >= startLayer + 1:\n",
        "    break\n",
        "  for i in range(0, npArray.shape[0]):\n",
        "    o1 = np.count_nonzero(npArray[i])\n",
        "    percent = ((o1*100)/(npArray.shape[1]*npArray.shape[2]))\n",
        "    print(\"For channel {}, percentage nonzeros {}\".format(i+1, percent))\n",
        "  print(\"Shape of numpy array {}\".format(npArray.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf1wSKsSKl5A"
      },
      "source": [
        "filePath = \"/content/gdrive/MyDrive/goldenOutputs/outputs_convLayer\"\n",
        "startLayer, endLayer = 3, 4\n",
        "for layerNumber in range(startLayer, endLayer):\n",
        "  fileName = filePath + str(layerNumber) + \".npy\"\n",
        "  npArray = np.load(fileName)\n",
        "  print(\"Shape of npArray at layerNumber = {} is {}\".format(layerNumber, npArray.shape))\n",
        "  if layerNumber >= startLayer+1:\n",
        "    break\n",
        "  for img in range(0, npArray.shape[0]):\n",
        "    if img >= 1:\n",
        "      break\n",
        "    for i in range(0, npArray.shape[1]):\n",
        "      o1 = np.count_nonzero(npArray[img][i])\n",
        "      percent = float((o1*100)/(npArray.shape[2]*npArray.shape[3]))\n",
        "      print(\"For channel {}, percentage nonzeros {}\".format(i+1, percent))\n",
        "  print(\"Shape of numpy array {}\".format(npArray.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tysBBeOTeBkU"
      },
      "source": [
        "def performReLU(inputTensor):\n",
        "  \"\"\"\n",
        "    Args: inputTensor - Apply ReLU on this tensor and return the new tensor\n",
        "  \"\"\"\n",
        "  input_np = inputTensor()\n",
        "  for val in np.nditer(input_np, op_flags=['readwrite']):\n",
        "    val[...] = max(0, val)\n",
        "  return torch.tensor(input_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRqULn4cLaLP"
      },
      "source": [
        "npArray = np.load(\"/content/gdrive/MyDrive/quantizedOutputs/activationsLayer1.npy\")\n",
        "activationTensor = torch.tensor(npArray)\n",
        "postReLUTensor = performReLU(activationTensor)\n",
        "for channel in range(0, postReLUTensor.shape[0]):\n",
        "  value = torch.count_nonzero(postReLUTensor[channel])\n",
        "  percent = float(value/(postReLUTensor.shape[1] * postReLUTensor.shape[2]))\n",
        "  print(\"For channel {}, percent {}\".format(channel, percent))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}